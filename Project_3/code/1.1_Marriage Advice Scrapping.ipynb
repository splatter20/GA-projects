{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d433ccc2",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: NLP classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce63d2f",
   "metadata": {},
   "source": [
    "## Part 1.1 - Data Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca189c9",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26c8e8",
   "metadata": {},
   "source": [
    "As a marriage counsellor, I want to reach out to couples who are struggling in their marriage and couples who are planning to get married. However, as marriage counselling is still something that is generally frowned upon, couples are still reluctant and hesitant to reach out for a counselling session.\n",
    "\n",
    "My goal is to go through 2 subreddits (Marriage Advice and Relationships) and to identify what are the different problems that are plaguing married couples and other unmarried couples and what are the different topics which they are seeking advice from.\n",
    "\n",
    "Subsequently, I will create an FAQ for both married couples and couples who are planning to get married, the FAQ will cover topics which are commonly raised in the subreddit threads. Futhermore, I will recommend that the couples attend a counselling session if they were to click on to some of the more worrying topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libaries for pushshift api\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba9320",
   "metadata": {},
   "source": [
    "### Data Scrapping of Marriage Advice Subreddit\n",
    "\n",
    "\n",
    "I will be using pushshift API to extract the posts from Reddit. \n",
    "\n",
    "As pushshift API is currently limiting 100 post per request, in order extract 10000 posts, I created a function to assist in extracting the posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the function\n",
    "def reddit_pushshift(reps, utc_int=None, subreddit, final_df):\n",
    "    \n",
    "    # Setting the initial time of the post\n",
    "    last_utc = utc_int\n",
    "    \n",
    "    # For loop to extract the post until the desired number of posts\n",
    "    for rep in range(reps):\n",
    "        \n",
    "        # Setting the base url and params\n",
    "        url = 'https://api.pushshift.io//reddit/search/submission/'\n",
    "        params = {\n",
    "        'subreddit': subreddit,\n",
    "        'size': 100,\n",
    "        'before': last_utc\n",
    "        }\n",
    "        \n",
    "        res = requests.get(url, params)\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "\n",
    "        # Create dataframe of posts with desired columns\n",
    "        df = pd.DataFrame(posts)\n",
    "\n",
    "        df = df[['selftext', 'title', 'created_utc', 'id', 'score']]\n",
    "        \n",
    "        # Concatenate dataframe to an existing dataframe\n",
    "        final_df = pd.concat(\n",
    "            objs=[final_df, df],\n",
    "            axis=0,\n",
    "        )\n",
    "        \n",
    "        # Resetting the index after concatenating\n",
    "        final_df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Reset the time factor to the time of last post\n",
    "        last_utc = final_df['created_utc'][-1:]\n",
    "        \n",
    "        # To check on the progress of the extraction\n",
    "        print(rep, last_utc)\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707be15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty dataframe to append the extracted posts.\n",
    "marriage_df = pd.DataFrame(columns = ['selftext', 'title', 'created_utc', 'id', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09faa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "marriage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edeb7b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract 100 post 100 times.\n",
    "marriage_df100 = reddit_pushshift(100, None, 'marriageadvice',  marriage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba39b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on the posts that were extracted\n",
    "marriage_df100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a6fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv for data cleaning later\n",
    "marriage_df100.to_csv('Marriage_Advice_all_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f02eb",
   "metadata": {},
   "source": [
    "### Proceed to next notebook to extract Relationship subreddit posts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
